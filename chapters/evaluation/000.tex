\chapter{Evaluation}\label{chap:evaluation}

\section{Cluster Purity}

Our core measure is \textit{cluster purity}. In \cite{DBLP:conf/bibm/McGovernDKBVG15} we
described the \mst{} process as essentially a classification task, where the class
labels are the names of the \spec{}. A \cplop{} bacterial strain (or cluster) may
contain \isols{} obtained from a single \spec{}, or from multiple \spec{}. 
A \textit{100\% pure cluster} is a cluster which only contains data points (\isols{}) with the same
class label (same species of origin). 

Consider a cluster $C=\{c_1,\ldots, c_K\}$. Let $s(c)$ refer to the species of isolate $c$.
Let $m$ be the plurality species label for data points in $C$, and let the total number of points in
$C$ with $s(c) = m$ be $s_m$. Then the \textit{individual cluster purity} $\nu$ of cluster $C$ is:
\[
    \nu(C) = \frac{s_m}{K}
\]

In addition to computing the purity of individual clusters
we want to have an understanding of the overall purity on the entire dataset.
Given a \textit{clustering} $\mathcal{C} = \{C_1,\dots,C_n\}$ on a dataset, we define the size $\mathcal{M}$ of the set of clusters: 
\begin{equation}\label{eq:num_isols}
\mathcal{M} = \sum_{i = 0}^{n} |C_i|
\end{equation}
The \textit{overall clustering purity} is:
\begin{equation}\label{eq:overall_clustering}
\sum_{i=1}^{n} \frac{|C_i|}{\mathcal{M}}\cdot\nu(C_i)
\end{equation}
One can think of \eqref{eq:overall_clustering} as a form of weighted arithmetic mean of the purities, where the size of the cluster adds more weight to the value.

% COVERAGE --------------------
\subsection{Coverage} \label{sec:validation:coverage}
Coverage of the dataset is important to an effective \mst{} method.
Density-based clustering method we use has one key disadvantage: a clustering run 
with the parameter \minneigh{}, treats all points that do not fit into a cluster of size 
of at least \minneigh{} as noise. This means that as the value of \minneigh{} grows,
so will the number of \isols{} without a strain. 

Given the parameter \minneigh{} of the clustering algorithm, we collect the following
four measures, that collectively represent the breakdown of all data points (\isols{}) in \cplop{}:
\begin{enumerate}
    \item \textit{Noise.} Number/percentage of \isols{} clustered as noise points.
    \item \textit{Misses.} Number/percentage of \isols{} from  minority species
    in impure clusters.
     \item \textit{Hits.} Number/percentage of \isols{} from plurality species in
     impure clusters.
     \item \textit{Pure points.} Number/percentage of \isols{} in 100\% pure clusters.
\end{enumerate}

\section{Classification}

There are a few areas of focus that we have when interpreting the results of \krap{}:
\begin{itemize}
\item What size $k$ achieves the best results?
\item What size $\alpha$ achieves the best results?
\item Which metric resolution algorithm achieves the best results?
\end{itemize}
Indeed we can define ``best'' in many ways, but we choose to look at two metrics, recall and precision, and a combination of the two, the $F$-measure. The metrics look at the accuracy of the classification on the object and the object on the classification respectively, while $F$-measure hopes to represent a balance between the two. 

\subsection{Cross Validation with Holdout}
To gauge accuracy of the results, we cross-validated against the library by separately holding out each \isol{} in CPLOP from CPLOP, classifying it against CPLOP, and verifying whether it is correct. Since each \isol{} in CPLOP has the correct \spec{}, we know whether a classification is correct or not.

\subsection{Recall}
In our study recall tracks how well we are able to discover all isolates from a given category, i.e.,
with a given host species. Given a category/host species name, the recall for that host species
is the percentage of isolates taken from this host species that have been properly identified.
For example, if our database had 100 cat isolates, and 74 of them were classified by our
method as having come from a cat, the recall would be 74\%. In this study, we compute both overall
recall (what percentage of isolates were classified as their proper label) as well as 
species-level recall (what percentage of isolates that came from dogs/humans/sheep/etc. were classified
as their proper label).

\subsection{Precision}
Precision tracks how well our method avoids misclassification errors. Given a category and a list
of isolates our method classified as belonging to it, the precision of the method on the
category is the percent of isolates from the list that has the correct label. For example,
if our method returned 100 isolates labelled ``Dog'' of which 77 isolates really did come
from dogs, the precision of the method is 77\%. As with recall, we compute both
overall precision, as well as the precision for each category/species label.

\subsection{\textit{F}-Measure}
The $F$-measure, $F_1$, is the \textit{harmonic mean} of the precision, $P$ and the recall, $R$:
\begin{equation*}
    F_1 
    =
    \frac{2}{\frac{1}{P}
    +
    \frac{1}{R}}
    = 2\cdot
    \frac{P\cdot R}
    {P + R}
\end{equation*}

While we prefer maximizing this value, a value near 0.5 means we are doing well.
